# BookNLP Container - CUDA/GPU Version
# Multi-stage build for optimized image with GPU support
#
# Build: DOCKER_BUILDKIT=1 docker build -f Dockerfile.gpu -t booknlp:cuda .
# Run:   docker run --gpus all -p 8000:8000 booknlp:cuda
#
# Requires: NVIDIA Container Toolkit on host
#
# Layer ordering optimized for cache efficiency:
# 1. System deps (rarely change)
# 2. Python deps with CUDA (change occasionally)
# 3. Models (change rarely, slow to download)
# 4. Source code (changes frequently, fast to copy)

# =============================================================================
# Stage 1: Dependencies - Install Python packages with CUDA support
# =============================================================================
FROM nvidia/cuda:12.4.1-runtime-ubuntu22.04 AS deps

WORKDIR /app

# Install Python 3.10 and build dependencies (Ubuntu 22.04 default)
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.10 \
    python3.10-venv \
    python3-distutils \
    python3-pip \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/* \
    && ln -sf /usr/bin/python3.10 /usr/bin/python3 \
    && ln -sf /usr/bin/python3.10 /usr/bin/python \
    && python3.10 -m venv /opt/venv

# Activate virtual environment
ENV PATH="/opt/venv/bin:$PATH"

# Upgrade pip in virtual environment
RUN pip install --no-cache-dir --upgrade pip

# Install Python dependencies with CUDA support in virtual environment
# Use BuildKit cache mount to persist pip downloads between builds
COPY requirements.txt .
RUN --mount=type=cache,target=/opt/venv/cache/pip \
    pip install -r requirements.txt

# Install PyTorch with CUDA 12.4 support (override CPU version from requirements)
RUN --mount=type=cache,target=/opt/venv/cache/pip \
    pip install torch==2.5.1 --index-url https://download.pytorch.org/whl/cu124

# =============================================================================
# Stage 2: Models - Download pretrained models
# =============================================================================
FROM deps AS models

# Install spacy model directly via pip in virtual environment
RUN --mount=type=cache,target=/opt/venv/cache/pip \
    pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl

# Install booknlp for model download (minimal source needed)
COPY setup.py .
COPY booknlp/ booknlp/
RUN --mount=type=cache,target=/opt/venv/cache/pip \
    pip install -e .

# Pre-download BookNLP models (both big and small)
# This triggers the automatic download from UC Berkeley servers
RUN python -c "from booknlp.booknlp import BookNLP; BookNLP('en', {'pipeline': 'entity', 'model': 'big'})" || true
RUN python -c "from booknlp.booknlp import BookNLP; BookNLP('en', {'pipeline': 'entity', 'model': 'small'})" || true

# =============================================================================
# Stage 3: Runtime - Final image with GPU support
# =============================================================================
FROM nvidia/cuda:12.4.1-runtime-ubuntu22.04 AS runtime

WORKDIR /app

# Install Python 3.10 and runtime dependencies (Ubuntu 22.04 default)
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.10 \
    python3.10-venv \
    python3-distutils \
    python3-pip \
    libgomp1 \
    curl \
    && rm -rf /var/lib/apt/lists/* \
    && ln -sf /usr/bin/python3.10 /usr/bin/python3 \
    && ln -sf /usr/bin/python3.10 /usr/bin/python

# Create virtual environment and activate
RUN python3.10 -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Copy Python packages from deps stage
COPY --from=deps /opt/venv /opt/venv

# Copy spacy model from models stage
COPY --from=models /opt/venv /opt/venv

# Copy downloaded BookNLP models from models stage
COPY --from=models /root/booknlp_models /root/booknlp_models

# Create non-root user with access to models
RUN useradd -m booknlp && \
    mkdir -p /home/booknlp/booknlp_models && \
    cp -r /root/booknlp_models/* /home/booknlp/booknlp_models/ 2>/dev/null || true && \
    mkdir -p /app/input /app/output && \
    chown -R booknlp:booknlp /app/input /app/output /home/booknlp

# Copy application source LAST (changes most frequently)
COPY --chown=booknlp:booknlp booknlp/ booknlp/
COPY --chown=booknlp:booknlp setup.py .

# Environment variables
ENV BOOKNLP_MODEL_PATH=/home/booknlp/booknlp_models
ENV BOOKNLP_DEFAULT_MODEL=small
ENV PYTHONPATH=/app
# CUDA environment
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility

# Expose API port
EXPOSE 8000

# Switch to non-root user
USER booknlp

# Healthcheck for container orchestration
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8000/v1/health || exit 1

# Default command - run API server
CMD ["python3", "-m", "uvicorn", "booknlp.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
